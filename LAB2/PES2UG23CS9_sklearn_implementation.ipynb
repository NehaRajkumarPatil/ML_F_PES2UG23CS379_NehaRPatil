{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caq5mToSrfAm",
        "outputId": "ac4aabcd-1de9-4ae4-b711-fcb386bdfeaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Framework: NumPy/sklearn\n",
            " Using NumPy arrays for all data processing\n",
            " Successfully imported module: EC_F_PES2UG23CS379_Lab3\n",
            " All required functions imported successfully\n",
            " DECISION TREE EVALUATION - NUMPY FRAMEWORK\n",
            "============================================================\n",
            " Datasets to process: 3\n",
            " Framework: NumPy/sklearn\n",
            "\n",
            " Processing 1/3: Mushrooms\n",
            "\n",
            "================================================================================\n",
            " TESTING DATASET: MUSHROOMS\n",
            " File: mushrooms.csv\n",
            " Framework: NumPy\n",
            "================================================================================\n",
            " Successfully loaded mushrooms.csv\n",
            " Target column: 'class' (last column)\n",
            " Original dataset info:\n",
            "Shape: (8124, 23)\n",
            "Columns: ['cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor', 'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color', 'stalk-shape', 'stalk-root', 'stalk-surface-above-ring', 'stalk-surface-below-ring', 'stalk-color-above-ring', 'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number', 'ring-type', 'spore-print-color', 'population', 'habitat', 'class']\n",
            "\n",
            "First few encoded samples:\n",
            "  cap-shape: ['x' 'b' 's' 'f' 'k'] → [5 0 4 2 3]\n",
            "  cap-surface: ['s' 'y' 'f' 'g'] → [2 3 0 1]\n",
            "  cap-color: ['n' 'y' 'w' 'g' 'e'] → [4 9 8 3 2]\n",
            "  class: ['p' 'e'] → [1 0]\n",
            "\n",
            " DATASET SUMMARY\n",
            "========================================\n",
            "Shape: (8124, 23)\n",
            "Features: 22\n",
            "Data type: <class 'numpy.ndarray'>\n",
            "Classes: 2 → [0, 1]\n",
            "Distribution: {np.int64(0): np.int64(4208), np.int64(1): np.int64(3916)}\n",
            "\n",
            " DECISION TREE CONSTRUCTION\n",
            "========================================\n",
            "Total samples: 8,124\n",
            "Training: 6,499 samples\n",
            "Testing: 1,625 samples\n",
            " Building tree (max depth: 8)...\n",
            " Decision tree construction completed!\n",
            "\n",
            " OVERALL PERFORMANCE METRICS\n",
            "========================================\n",
            "Accuracy:             1.0000 (100.00%)\n",
            "Precision (weighted): 1.0000\n",
            "Recall (weighted):    1.0000\n",
            "F1-Score (weighted):  1.0000\n",
            "Precision (macro):    1.0000\n",
            "Recall (macro):       1.0000\n",
            "F1-Score (macro):     1.0000\n",
            "\n",
            " TREE COMPLEXITY METRICS\n",
            "========================================\n",
            "Maximum Depth:        4\n",
            "Total Nodes:          29\n",
            "Leaf Nodes:           24\n",
            "Internal Nodes:       5\n",
            "\n",
            " Processing 2/3: Tic-Tac-Toe\n",
            "\n",
            "================================================================================\n",
            " TESTING DATASET: TIC-TAC-TOE\n",
            " File: tictactoe.csv\n",
            " Framework: NumPy\n",
            "================================================================================\n",
            " Successfully loaded tictactoe.csv\n",
            " Target column: 'Class' (last column)\n",
            " Original dataset info:\n",
            "Shape: (958, 10)\n",
            "Columns: ['top-left-square', 'top-middle-square', 'top-right-square', 'middle-left-square', 'middle-middle-square', 'middle-right-square', 'bottom-left-square', 'bottom-middle-square', 'bottom-right-square', 'Class']\n",
            "\n",
            "First few encoded samples:\n",
            "  top-left-square: ['x' 'o' 'b'] → [2 1 0]\n",
            "  top-middle-square: ['x' 'o' 'b'] → [2 1 0]\n",
            "  top-right-square: ['x' 'o' 'b'] → [2 1 0]\n",
            "  Class: ['positive' 'negative'] → [1 0]\n",
            "\n",
            " DATASET SUMMARY\n",
            "========================================\n",
            "Shape: (958, 10)\n",
            "Features: 9\n",
            "Data type: <class 'numpy.ndarray'>\n",
            "Classes: 2 → [0, 1]\n",
            "Distribution: {np.int64(0): np.int64(332), np.int64(1): np.int64(626)}\n",
            "\n",
            " DECISION TREE CONSTRUCTION\n",
            "========================================\n",
            "Total samples: 958\n",
            "Training: 766 samples\n",
            "Testing: 192 samples\n",
            " Building tree (max depth: 5)...\n",
            " Decision tree construction completed!\n",
            "\n",
            " OVERALL PERFORMANCE METRICS\n",
            "========================================\n",
            "Accuracy:             0.8594 (85.94%)\n",
            "Precision (weighted): 0.8589\n",
            "Recall (weighted):    0.8594\n",
            "F1-Score (weighted):  0.8591\n",
            "Precision (macro):    0.8449\n",
            "Recall (macro):       0.8424\n",
            "F1-Score (macro):     0.8436\n",
            "\n",
            " TREE COMPLEXITY METRICS\n",
            "========================================\n",
            "Maximum Depth:        5\n",
            "Total Nodes:          145\n",
            "Leaf Nodes:           96\n",
            "Internal Nodes:       49\n",
            "\n",
            " Processing 3/3: Nursery\n",
            "\n",
            "================================================================================\n",
            " TESTING DATASET: NURSERY\n",
            " File: Nursery.csv\n",
            " Framework: NumPy\n",
            "================================================================================\n",
            " Successfully loaded Nursery.csv\n",
            " Target column: 'class' (last column)\n",
            " Original dataset info:\n",
            "Shape: (12960, 9)\n",
            "Columns: ['parents', 'has_nurs', 'form', 'children', 'housing', 'finance', 'social', 'health', 'class']\n",
            "\n",
            "First few encoded samples:\n",
            "  parents: ['usual' 'pretentious' 'great_pret'] → [2 1 0]\n",
            "  has_nurs: ['proper' 'less_proper' 'improper' 'critical' 'very_crit'] → [3 2 1 0 4]\n",
            "  form: ['complete' 'completed' 'incomplete' 'foster'] → [0 1 3 2]\n",
            "  class: ['recommend' 'priority' 'not_recom' 'very_recom' 'spec_prior'] → [2 1 0 4 3]\n",
            "\n",
            " DATASET SUMMARY\n",
            "========================================\n",
            "Shape: (12960, 9)\n",
            "Features: 8\n",
            "Data type: <class 'numpy.ndarray'>\n",
            "Classes: 5 → [0, 1, 2, 3, 4]\n",
            "Distribution: {np.int64(0): np.int64(4320), np.int64(1): np.int64(4266), np.int64(2): np.int64(2), np.int64(3): np.int64(4044), np.int64(4): np.int64(328)}\n",
            "\n",
            " DECISION TREE CONSTRUCTION\n",
            "========================================\n",
            "Total samples: 12,960\n",
            "Training: 10,368 samples\n",
            "Testing: 2,592 samples\n",
            " Building tree (max depth: 5)...\n",
            " Decision tree construction completed!\n",
            "\n",
            " OVERALL PERFORMANCE METRICS\n",
            "========================================\n",
            "Accuracy:             0.9367 (93.67%)\n",
            "Precision (weighted): 0.9389\n",
            "Recall (weighted):    0.9367\n",
            "F1-Score (weighted):  0.9375\n",
            "Precision (macro):    0.8312\n",
            "Recall (macro):       0.8598\n",
            "F1-Score (macro):     0.8435\n",
            "\n",
            " TREE COMPLEXITY METRICS\n",
            "========================================\n",
            "Maximum Depth:        5\n",
            "Total Nodes:          272\n",
            "Leaf Nodes:           190\n",
            "Internal Nodes:       82\n",
            "\n",
            "====================================================================================================\n",
            " COMPREHENSIVE SUMMARY REPORT - NUMPY FRAMEWORK\n",
            "====================================================================================================\n",
            "\n",
            " SUCCESSFUL EVALUATIONS: 3/3\n",
            "================================================================================\n",
            "Dataset         Accuracy   Precision  Recall     F1-Score   Depth   Nodes  \n",
            "--------------------------------------------------------------------------------\n",
            "Mushrooms       1.0000     1.0000     1.0000     1.0000     4       29     \n",
            "Tic-Tac-Toe     0.8594     0.8589     0.8594     0.8591     5       145    \n",
            "Nursery         0.9367     0.9389     0.9367     0.9375     5       272    \n",
            "\n",
            " BEST ACCURACY: Mushrooms (100.0%)\n",
            "\n",
            " DATASET COMPLEXITY ANALYSIS\n",
            "==================================================\n",
            "\n",
            "Mushrooms:\n",
            "  • Size: 8,124 samples × 22 features\n",
            "  • Classes: 2 classes\n",
            "  • Tree: 4 levels, 29 nodes\n",
            "  • Performance: 100.0% accuracy\n",
            "\n",
            "Tic-Tac-Toe:\n",
            "  • Size: 958 samples × 9 features\n",
            "  • Classes: 2 classes\n",
            "  • Tree: 5 levels, 145 nodes\n",
            "  • Performance: 85.9% accuracy\n",
            "\n",
            "Nursery:\n",
            "  • Size: 12,960 samples × 8 features\n",
            "  • Classes: 5 classes\n",
            "  • Tree: 5 levels, 272 nodes\n",
            "  • Performance: 93.7% accuracy\n",
            "\n",
            "====================================================================================================\n",
            "\n",
            " EVALUATION COMPLETE!\n",
            "Framework: NumPy\n",
            "Datasets processed: 3\n",
            "Successful: 3\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import importlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configuration\n",
        "subname = \"EC_F_PES2UG23CS379_Lab3\"  # Replace with your actual module name\n",
        "datasets = [\n",
        "    {\"name\": \"Mushrooms\", \"path\": \"mushrooms.csv\"},\n",
        "    {\"name\": \"Tic-Tac-Toe\", \"path\": \"tictactoe.csv\"},\n",
        "    {\"name\": \"Nursery\", \"path\": \"Nursery.csv\"}\n",
        "]\n",
        "\n",
        "framework = 'numpy'  # Using NumPy framework\n",
        "print_tree_flag = False\n",
        "print_construction_flag = False\n",
        "\n",
        "print(f\" Framework: NumPy/sklearn\")\n",
        "print(f\" Using NumPy arrays for all data processing\")\n",
        "\n",
        "# Import your module\n",
        "try:\n",
        "    mymodule = importlib.import_module(subname)\n",
        "    print(f\" Successfully imported module: {subname}\")\n",
        "except Exception as e:\n",
        "    print(f\" Error importing module '{subname}': {e}\")\n",
        "    print(\"Please ensure your module is named correctly and is available.\")\n",
        "    sys.exit()\n",
        "\n",
        "# Import required functions from your module\n",
        "try:\n",
        "    get_selected_attribute = mymodule.get_selected_attribute\n",
        "    get_information_gain = mymodule.get_information_gain\n",
        "    get_avg_info_of_attribute = mymodule.get_avg_info_of_attribute\n",
        "    get_entropy_of_dataset = mymodule.get_entropy_of_dataset\n",
        "    print(\" All required functions imported successfully\")\n",
        "except AttributeError as e:\n",
        "    print(f\" Error: Missing required function in module '{subname}': {e}\")\n",
        "    print(\"Required functions: get_selected_attribute, get_information_gain, get_avg_info_of_attribute, get_entropy_of_dataset\")\n",
        "    sys.exit()\n",
        "\n",
        "def calculate_accuracy(y_true, y_pred):\n",
        "    \"\"\"Calculate accuracy with NumPy arrays\"\"\"\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # Handle None predictions\n",
        "    valid_mask = np.array([pred is not None for pred in y_pred])\n",
        "    if not np.any(valid_mask):\n",
        "        return 0.0\n",
        "\n",
        "    y_true_valid = y_true[valid_mask]\n",
        "    y_pred_valid = y_pred[valid_mask]\n",
        "\n",
        "    correct = np.sum(y_true_valid == y_pred_valid)\n",
        "    total = len(y_true_valid)\n",
        "\n",
        "    return correct / total if total > 0 else 0.0\n",
        "\n",
        "def calculate_precision_recall_f1(y_true, y_pred, average='weighted'):\n",
        "    \"\"\"Calculate precision, recall, and F1-score with NumPy\"\"\"\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # Handle None predictions\n",
        "    valid_mask = np.array([pred is not None for pred in y_pred])\n",
        "    if not np.any(valid_mask):\n",
        "        return 0.0, 0.0, 0.0\n",
        "\n",
        "    y_true_valid = y_true[valid_mask]\n",
        "    y_pred_valid = y_pred[valid_mask]\n",
        "\n",
        "    classes = np.unique(np.concatenate([y_true_valid, y_pred_valid]))\n",
        "\n",
        "    if average == 'weighted':\n",
        "        precisions, recalls, f1s, supports = [], [], [], []\n",
        "\n",
        "        for cls in classes:\n",
        "            tp = np.sum((y_true_valid == cls) & (y_pred_valid == cls))\n",
        "            fp = np.sum((y_true_valid != cls) & (y_pred_valid == cls))\n",
        "            fn = np.sum((y_true_valid == cls) & (y_pred_valid != cls))\n",
        "\n",
        "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "            support = np.sum(y_true_valid == cls)\n",
        "\n",
        "            precisions.append(precision)\n",
        "            recalls.append(recall)\n",
        "            f1s.append(f1)\n",
        "            supports.append(support)\n",
        "\n",
        "        total_support = sum(supports)\n",
        "        if total_support == 0:\n",
        "            return 0.0, 0.0, 0.0\n",
        "\n",
        "        weighted_precision = sum(p * s for p, s in zip(precisions, supports)) / total_support\n",
        "        weighted_recall = sum(r * s for r, s in zip(recalls, supports)) / total_support\n",
        "        weighted_f1 = sum(f * s for f, s in zip(f1s, supports)) / total_support\n",
        "\n",
        "        return weighted_precision, weighted_recall, weighted_f1\n",
        "\n",
        "    elif average == 'macro':\n",
        "        precisions, recalls, f1s = [], [], []\n",
        "\n",
        "        for cls in classes:\n",
        "            tp = np.sum((y_true_valid == cls) & (y_pred_valid == cls))\n",
        "            fp = np.sum((y_true_valid != cls) & (y_pred_valid == cls))\n",
        "            fn = np.sum((y_true_valid == cls) & (y_pred_valid != cls))\n",
        "\n",
        "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "            precisions.append(precision)\n",
        "            recalls.append(recall)\n",
        "            f1s.append(f1)\n",
        "\n",
        "        return np.mean(precisions), np.mean(recalls), np.mean(f1s)\n",
        "\n",
        "def calculate_per_class_metrics(y_true, y_pred):\n",
        "    \"\"\"Calculate per-class metrics with NumPy\"\"\"\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    valid_mask = np.array([pred is not None for pred in y_pred])\n",
        "    if not np.any(valid_mask):\n",
        "        return {}\n",
        "\n",
        "    y_true_valid = y_true[valid_mask]\n",
        "    y_pred_valid = y_pred[valid_mask]\n",
        "\n",
        "    classes = np.unique(np.concatenate([y_true_valid, y_pred_valid]))\n",
        "    metrics = {}\n",
        "\n",
        "    for cls in classes:\n",
        "        tp = np.sum((y_true_valid == cls) & (y_pred_valid == cls))\n",
        "        fp = np.sum((y_true_valid != cls) & (y_pred_valid == cls))\n",
        "        fn = np.sum((y_true_valid == cls) & (y_pred_valid != cls))\n",
        "        tn = np.sum((y_true_valid != cls) & (y_pred_valid != cls))\n",
        "\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
        "        support = np.sum(y_true_valid == cls)\n",
        "\n",
        "        metrics[cls] = {\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1,\n",
        "            'specificity': specificity,\n",
        "            'support': support,\n",
        "            'true_positives': tp,\n",
        "            'false_positives': fp,\n",
        "            'false_negatives': fn,\n",
        "            'true_negatives': tn\n",
        "        }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def calculate_tree_complexity_metrics(tree):\n",
        "    \"\"\"Calculate tree complexity metrics\"\"\"\n",
        "    if isinstance(tree, (int, np.integer)) or tree is None:\n",
        "        return {\n",
        "            'max_depth': 0,\n",
        "            'num_nodes': 1,\n",
        "            'num_leaves': 1,\n",
        "            'num_internal_nodes': 0\n",
        "        }\n",
        "\n",
        "    if not isinstance(tree, dict):\n",
        "        return {\n",
        "            'max_depth': 0,\n",
        "            'num_nodes': 1,\n",
        "            'num_leaves': 1,\n",
        "            'num_internal_nodes': 0\n",
        "        }\n",
        "\n",
        "    def get_tree_stats(node, depth=0):\n",
        "        if isinstance(node, (int, np.integer)) or node is None:\n",
        "            return {\n",
        "                'max_depth': depth,\n",
        "                'num_nodes': 1,\n",
        "                'num_leaves': 1,\n",
        "                'num_internal_nodes': 0\n",
        "            }\n",
        "\n",
        "        if not isinstance(node, dict) or 'branches' not in node:\n",
        "            return {\n",
        "                'max_depth': depth,\n",
        "                'num_nodes': 1,\n",
        "                'num_leaves': 1,\n",
        "                'num_internal_nodes': 0\n",
        "            }\n",
        "\n",
        "        max_depth = depth\n",
        "        total_nodes = 1\n",
        "        total_leaves = 0\n",
        "        total_internal = 1\n",
        "\n",
        "        for branch_value, subtree in node['branches'].items():\n",
        "            subtree_stats = get_tree_stats(subtree, depth + 1)\n",
        "            max_depth = max(max_depth, subtree_stats['max_depth'])\n",
        "            total_nodes += subtree_stats['num_nodes']\n",
        "            total_leaves += subtree_stats['num_leaves']\n",
        "            total_internal += subtree_stats['num_internal_nodes']\n",
        "\n",
        "        return {\n",
        "            'max_depth': max_depth,\n",
        "            'num_nodes': total_nodes,\n",
        "            'num_leaves': total_leaves,\n",
        "            'num_internal_nodes': total_internal\n",
        "        }\n",
        "\n",
        "    return get_tree_stats(tree)\n",
        "\n",
        "def evaluate_decision_tree(tree, X_test, y_test, cols, class_names=None):\n",
        "    \"\"\"Evaluate decision tree performance\"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    # Make predictions for all test samples\n",
        "    for sample in X_test:\n",
        "        pred = predict_single_sample(tree, sample, cols)\n",
        "        predictions.append(pred)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = calculate_accuracy(y_test, predictions)\n",
        "    precision, recall, f1 = calculate_precision_recall_f1(y_test, predictions, average='weighted')\n",
        "    precision_macro, recall_macro, f1_macro = calculate_precision_recall_f1(y_test, predictions, average='macro')\n",
        "\n",
        "    per_class_metrics = calculate_per_class_metrics(y_test, predictions)\n",
        "    tree_stats = calculate_tree_complexity_metrics(tree)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\n OVERALL PERFORMANCE METRICS\")\n",
        "    print(f\"{'='*40}\")\n",
        "    print(f\"Accuracy:             {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "    print(f\"Precision (weighted): {precision:.4f}\")\n",
        "    print(f\"Recall (weighted):    {recall:.4f}\")\n",
        "    print(f\"F1-Score (weighted):  {f1:.4f}\")\n",
        "    print(f\"Precision (macro):    {precision_macro:.4f}\")\n",
        "    print(f\"Recall (macro):       {recall_macro:.4f}\")\n",
        "    print(f\"F1-Score (macro):     {f1_macro:.4f}\")\n",
        "\n",
        "    print(f\"\\n TREE COMPLEXITY METRICS\")\n",
        "    print(f\"{'='*40}\")\n",
        "    print(f\"Maximum Depth:        {tree_stats['max_depth']}\")\n",
        "    print(f\"Total Nodes:          {tree_stats['num_nodes']}\")\n",
        "    print(f\"Leaf Nodes:           {tree_stats['num_leaves']}\")\n",
        "    print(f\"Internal Nodes:       {tree_stats['num_internal_nodes']}\")\n",
        "\n",
        "    total_predictions = len(predictions)\n",
        "    valid_predictions = sum(1 for p in predictions if p is not None)\n",
        "    prediction_rate = valid_predictions / total_predictions if total_predictions > 0 else 0\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision_weighted': precision,\n",
        "        'recall_weighted': recall,\n",
        "        'f1_weighted': f1,\n",
        "        'precision_macro': precision_macro,\n",
        "        'recall_macro': recall_macro,\n",
        "        'f1_macro': f1_macro,\n",
        "        'per_class_metrics': per_class_metrics,\n",
        "        'tree_complexity': tree_stats,\n",
        "        'predictions': predictions,\n",
        "        'prediction_rate': prediction_rate\n",
        "    }\n",
        "\n",
        "def preprocess_data(df):\n",
        "    \"\"\"Preprocess dataset by encoding categorical variables\"\"\"\n",
        "    df_processed = df.copy()\n",
        "\n",
        "    print(\" Original dataset info:\")\n",
        "    print(f\"Shape: {df.shape}\")\n",
        "    print(f\"Columns: {list(df.columns)}\")\n",
        "    print(\"\\nFirst few encoded samples:\")\n",
        "\n",
        "    label_encoders = {}\n",
        "    for column in df_processed.columns:\n",
        "        le = LabelEncoder()\n",
        "        df_processed[column] = le.fit_transform(df_processed[column])\n",
        "        label_encoders[column] = le\n",
        "\n",
        "        # Show encoding for first few columns and target\n",
        "        if column in list(df.columns)[:3] or column == df.columns[-1]:\n",
        "            unique_orig = df[column].unique()[:5]\n",
        "            unique_encoded = df_processed[column].unique()[:5]\n",
        "            print(f\"  {column}: {unique_orig} → {unique_encoded}\")\n",
        "\n",
        "    return df_processed, label_encoders\n",
        "\n",
        "def construct_tree(data, cols, used_attributes=None, level=0, max_depth=6, print_construction=False):\n",
        "    \"\"\"\n",
        "    Construct decision tree using ID3 algorithm with NumPy\n",
        "\n",
        "    Args:\n",
        "        data: NumPy array with features and target (last column)\n",
        "        cols: List of column names\n",
        "        used_attributes: Set of already used attribute indices\n",
        "        level: Current tree depth level\n",
        "        max_depth: Maximum allowed depth\n",
        "        print_construction: Whether to print construction details\n",
        "\n",
        "    Returns:\n",
        "        Tree structure (dict for internal nodes, int for leaf nodes)\n",
        "    \"\"\"\n",
        "    if used_attributes is None:\n",
        "        used_attributes = set()\n",
        "\n",
        "    # Base case: empty data\n",
        "    if len(data) == 0:\n",
        "        return None\n",
        "\n",
        "    # Ensure data is NumPy array\n",
        "    data = np.array(data, dtype=np.float32)\n",
        "\n",
        "    # Calculate entropy of current dataset\n",
        "    entropy = get_entropy_of_dataset(data)\n",
        "    if print_construction:\n",
        "        print_node_info(f\"Entropy = {entropy:.4f}\", level)\n",
        "\n",
        "    # Base case: pure node (entropy ≈ 0)\n",
        "    if entropy < 1e-10:  # Using small threshold for floating point comparison\n",
        "        target_values = data[:, -1].astype(int)\n",
        "        majority_class = int(target_values[0])  # All same class\n",
        "        if print_construction:\n",
        "            print_node_info(f\"Leaf: Class {majority_class} (pure node)\", level)\n",
        "        return majority_class\n",
        "\n",
        "    # Base case: max depth reached\n",
        "    if level >= max_depth:\n",
        "        unique_vals, counts = np.unique(data[:, -1], return_counts=True)\n",
        "        majority_idx = np.argmax(counts)\n",
        "        majority_class = int(unique_vals[majority_idx])\n",
        "        if print_construction:\n",
        "            print_node_info(f\"Leaf: Class {majority_class} (max depth)\", level)\n",
        "        return majority_class\n",
        "\n",
        "    # Base case: no more attributes available\n",
        "    num_features = len(cols) - 1\n",
        "    if len(used_attributes) >= num_features:\n",
        "        unique_vals, counts = np.unique(data[:, -1], return_counts=True)\n",
        "        majority_idx = np.argmax(counts)\n",
        "        majority_class = int(unique_vals[majority_idx])\n",
        "        if print_construction:\n",
        "            print_node_info(f\"Leaf: Class {majority_class} (no attributes)\", level)\n",
        "        return majority_class\n",
        "\n",
        "    try:\n",
        "        # Get the best attribute to split on\n",
        "        gain_dict, selected_attribute = get_selected_attribute(data)\n",
        "\n",
        "        # Filter out already used attributes\n",
        "        available_gains = {attr: gain for attr, gain in gain_dict.items()\n",
        "                          if attr not in used_attributes}\n",
        "\n",
        "        # Base case: no available attributes or no information gain\n",
        "        if not available_gains or max(available_gains.values()) <= 0:\n",
        "            unique_vals, counts = np.unique(data[:, -1], return_counts=True)\n",
        "            majority_idx = np.argmax(counts)\n",
        "            majority_class = int(unique_vals[majority_idx])\n",
        "            if print_construction:\n",
        "                print_node_info(f\"Leaf: Class {majority_class} (no gain)\", level)\n",
        "            return majority_class\n",
        "\n",
        "        # Select the attribute with highest information gain\n",
        "        selected_attribute = max(available_gains, key=available_gains.get)\n",
        "\n",
        "        if print_construction:\n",
        "            print_node_info(f\"Split on: {cols[selected_attribute]} (gain: {available_gains[selected_attribute]:.4f})\", level)\n",
        "\n",
        "        # Create tree node\n",
        "        tree_node = {\n",
        "            'attribute': selected_attribute,\n",
        "            'attribute_name': cols[selected_attribute],\n",
        "            'gain': available_gains[selected_attribute],\n",
        "            'level': level,\n",
        "            'branches': {}\n",
        "        }\n",
        "\n",
        "        # Get unique values for the selected attribute\n",
        "        unique_values = np.unique(data[:, selected_attribute])\n",
        "        new_used_attributes = used_attributes.copy()\n",
        "        new_used_attributes.add(selected_attribute)\n",
        "\n",
        "        # Create branches for each unique value\n",
        "        for value in unique_values:\n",
        "            mask = data[:, selected_attribute] == value\n",
        "            subset_data = data[mask]\n",
        "\n",
        "            value_int = int(value)\n",
        "\n",
        "            # Handle empty subset\n",
        "            if len(subset_data) == 0:\n",
        "                unique_vals, counts = np.unique(data[:, -1], return_counts=True)\n",
        "                majority_idx = np.argmax(counts)\n",
        "                majority_class = int(unique_vals[majority_idx])\n",
        "                if print_construction:\n",
        "                    print_node_info(f\"Branch {cols[selected_attribute]} = {value_int} → Class {majority_class} (empty)\", level)\n",
        "                tree_node['branches'][value_int] = majority_class\n",
        "                continue\n",
        "\n",
        "            if print_construction:\n",
        "                print_node_info(f\"Branch {cols[selected_attribute]} = {value_int} ({len(subset_data)} samples)\", level)\n",
        "\n",
        "            # Recursive call\n",
        "            subtree = construct_tree(\n",
        "                subset_data,\n",
        "                cols,\n",
        "                new_used_attributes,\n",
        "                level + 1,\n",
        "                max_depth,\n",
        "                print_construction\n",
        "            )\n",
        "\n",
        "            tree_node['branches'][value_int] = subtree\n",
        "\n",
        "        return tree_node\n",
        "\n",
        "    except Exception as e:\n",
        "        if print_construction:\n",
        "            print_node_info(f\"Error: {e}\", level)\n",
        "\n",
        "        # Fallback to majority class\n",
        "        try:\n",
        "            unique_vals, counts = np.unique(data[:, -1], return_counts=True)\n",
        "            majority_idx = np.argmax(counts)\n",
        "            majority_class = int(unique_vals[majority_idx])\n",
        "            return majority_class\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "def predict_single_sample(tree, sample, cols):\n",
        "    \"\"\"\n",
        "    Predict class for a single sample using the decision tree\n",
        "\n",
        "    Args:\n",
        "        tree: Decision tree (dict or int)\n",
        "        sample: Single data sample (array-like)\n",
        "        cols: Column names\n",
        "\n",
        "    Returns:\n",
        "        Predicted class (int) or None if prediction fails\n",
        "    \"\"\"\n",
        "    # Base case: leaf node\n",
        "    if isinstance(tree, (int, np.integer)):\n",
        "        return int(tree)\n",
        "\n",
        "    # Invalid tree structure\n",
        "    if not isinstance(tree, dict) or 'attribute' not in tree:\n",
        "        return None\n",
        "\n",
        "    attribute_idx = tree['attribute']\n",
        "\n",
        "    # Check if attribute index is valid\n",
        "    if attribute_idx >= len(sample):\n",
        "        return None\n",
        "\n",
        "    attribute_value = int(sample[attribute_idx])\n",
        "\n",
        "    # Check if this attribute value exists in the tree\n",
        "    if attribute_value not in tree['branches']:\n",
        "        return None\n",
        "\n",
        "    # Recursively traverse the tree\n",
        "    subtree = tree['branches'][attribute_value]\n",
        "    return predict_single_sample(subtree, sample, cols)\n",
        "\n",
        "def print_tree_structure(tree, cols, level=0, prefix=\"\"):\n",
        "    \"\"\"Pretty print the tree structure\"\"\"\n",
        "    if isinstance(tree, (int, np.integer)):\n",
        "        print(f\"{prefix}├── Class {tree}\")\n",
        "        return\n",
        "\n",
        "    if not isinstance(tree, dict) or 'attribute' not in tree:\n",
        "        print(f\"{prefix}├── None\")\n",
        "        return\n",
        "\n",
        "    attr_name = tree['attribute_name']\n",
        "    gain = tree.get('gain', 0)\n",
        "\n",
        "    if level == 0:\n",
        "        print(f\"Root: {attr_name} (gain: {gain:.4f})\")\n",
        "\n",
        "    branches = tree['branches']\n",
        "    branch_items = list(branches.items())\n",
        "\n",
        "    for i, (value, subtree) in enumerate(branch_items):\n",
        "        is_last = (i == len(branch_items) - 1)\n",
        "\n",
        "        if level == 0:\n",
        "            print(f\"├── = {value}:\")\n",
        "            new_prefix = \"│   \"\n",
        "        else:\n",
        "            branch_symbol = \"└──\" if is_last else \"├──\"\n",
        "            print(f\"{prefix}{branch_symbol} = {value}:\")\n",
        "            new_prefix = prefix + (\"    \" if is_last else \"│   \")\n",
        "\n",
        "        if isinstance(subtree, (int, np.integer)):\n",
        "            print(f\"{new_prefix}├── Class {subtree}\")\n",
        "        elif isinstance(subtree, dict):\n",
        "            attr_name = subtree['attribute_name']\n",
        "            gain = subtree.get('gain', 0)\n",
        "            print(f\"{new_prefix}├── {attr_name} (gain: {gain:.4f})\")\n",
        "            print_tree_structure(subtree, cols, level + 1, new_prefix)\n",
        "        else:\n",
        "            print(f\"{new_prefix}├── None\")\n",
        "\n",
        "def print_node_info(message, level):\n",
        "    \"\"\"Print formatted node information\"\"\"\n",
        "    indent = \"  \" * level\n",
        "    print(f\"Level {level}: {indent}{message}\")\n",
        "\n",
        "def test_single_dataset(dataset_info):\n",
        "    \"\"\"Test a single dataset\"\"\"\n",
        "    dataset_name = dataset_info[\"name\"]\n",
        "    data_path = dataset_info[\"path\"]\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\" TESTING DATASET: {dataset_name.upper()}\")\n",
        "    print(f\" File: {data_path}\")\n",
        "    print(f\" Framework: NumPy\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    try:\n",
        "        # Load dataset\n",
        "        df = pd.read_csv(data_path)\n",
        "        print(f\" Successfully loaded {data_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\" Error loading dataset {data_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "    print(f\" Target column: '{df.columns[-1]}' (last column)\")\n",
        "\n",
        "    # Preprocess data\n",
        "    df_processed, label_encoders = preprocess_data(df)\n",
        "\n",
        "    # Convert to NumPy array\n",
        "    dataset = df_processed.values.astype(np.float32)\n",
        "    cols = list(df_processed.columns)\n",
        "\n",
        "    print(f\"\\n DATASET SUMMARY\")\n",
        "    print(f\"{'='*40}\")\n",
        "    print(f\"Shape: {dataset.shape}\")\n",
        "    print(f\"Features: {len(cols) - 1}\")\n",
        "    print(f\"Data type: {type(dataset)}\")\n",
        "\n",
        "    # Get class distribution\n",
        "    unique_classes, class_counts = np.unique(dataset[:, -1], return_counts=True)\n",
        "    unique_classes = unique_classes.astype(int)\n",
        "    class_counts = class_counts.astype(int)\n",
        "\n",
        "    print(f\"Classes: {len(unique_classes)} → {unique_classes.tolist()}\")\n",
        "    print(f\"Distribution: {dict(zip(unique_classes, class_counts))}\")\n",
        "\n",
        "    try:\n",
        "        print(f\"\\n DECISION TREE CONSTRUCTION\")\n",
        "        print(f\"{'='*40}\")\n",
        "\n",
        "        # Train-test split\n",
        "        total_samples = len(dataset)\n",
        "        train_split = 0.8\n",
        "        train_size = int(total_samples * train_split)\n",
        "\n",
        "        print(f\"Total samples: {total_samples:,}\")\n",
        "        print(f\"Training: {train_size:,} samples\")\n",
        "        print(f\"Testing: {total_samples - train_size:,} samples\")\n",
        "\n",
        "        # Shuffle and split data\n",
        "        np.random.seed(42)\n",
        "        indices = np.random.permutation(total_samples)\n",
        "        dataset_shuffled = dataset[indices]\n",
        "\n",
        "        train_data = dataset_shuffled[:train_size]\n",
        "        test_data = dataset_shuffled[train_size:]\n",
        "\n",
        "        # Adjust max depth based on dataset complexity\n",
        "        if len(cols) > 20:  # Very complex datasets\n",
        "            max_depth = 8\n",
        "        elif len(cols) > 15:  # Complex datasets\n",
        "            max_depth = 7\n",
        "        elif len(cols) > 10:  # Medium datasets\n",
        "            max_depth = 6\n",
        "        else:  # Simple datasets\n",
        "            max_depth = 5\n",
        "\n",
        "        print(f\" Building tree (max depth: {max_depth})...\")\n",
        "\n",
        "        # Construct decision tree\n",
        "        tree = construct_tree(\n",
        "            train_data,\n",
        "            cols=cols,\n",
        "            max_depth=max_depth,\n",
        "            print_construction=print_construction_flag\n",
        "        )\n",
        "\n",
        "        if tree is not None:\n",
        "            print(f\" Decision tree construction completed!\")\n",
        "\n",
        "            if print_tree_flag:\n",
        "                print(f\"\\n DECISION TREE STRUCTURE\")\n",
        "                print(\"=\"*60)\n",
        "                print_tree_structure(tree, cols)\n",
        "                print()\n",
        "\n",
        "            # Prepare test data\n",
        "            X_test = test_data[:, :-1]\n",
        "            y_test = test_data[:, -1]\n",
        "\n",
        "            # Get class names for reporting\n",
        "            target_col = cols[-1]\n",
        "            class_names = None\n",
        "            if target_col in label_encoders:\n",
        "                le = label_encoders[target_col]\n",
        "                class_names = {i: le.inverse_transform([i])[0] for i in range(len(le.classes_))}\n",
        "\n",
        "            # Evaluate the tree\n",
        "            evaluation_results = evaluate_decision_tree(tree, X_test, y_test, cols, class_names)\n",
        "\n",
        "            return {\n",
        "                'dataset_name': dataset_name,\n",
        "                'success': True,\n",
        "                'results': evaluation_results,\n",
        "                'dataset_info': {\n",
        "                    'shape': df.shape,\n",
        "                    'num_features': len(cols) - 1,\n",
        "                    'num_classes': len(unique_classes),\n",
        "                    'class_distribution': dict(zip(unique_classes, class_counts))\n",
        "                }\n",
        "            }\n",
        "\n",
        "        else:\n",
        "            print(\" Tree construction failed!\")\n",
        "            return {\n",
        "                'dataset_name': dataset_name,\n",
        "                'success': False,\n",
        "                'error': 'Tree construction returned None'\n",
        "            }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Error in tree construction: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return {\n",
        "            'dataset_name': dataset_name,\n",
        "            'success': False,\n",
        "            'error': str(e)\n",
        "        }\n",
        "\n",
        "def print_summary_report(all_results):\n",
        "    \"\"\"Print comprehensive summary report\"\"\"\n",
        "    print(f\"\\n{'='*100}\")\n",
        "    print(f\" COMPREHENSIVE SUMMARY REPORT - NUMPY FRAMEWORK\")\n",
        "    print(f\"{'='*100}\")\n",
        "\n",
        "    successful_results = [r for r in all_results if r['success']]\n",
        "    failed_results = [r for r in all_results if not r['success']]\n",
        "\n",
        "    if successful_results:\n",
        "        print(f\"\\n SUCCESSFUL EVALUATIONS: {len(successful_results)}/{len(all_results)}\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        # Results table\n",
        "        print(f\"{'Dataset':<15} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Depth':<7} {'Nodes':<7}\")\n",
        "        print(f\"{'-'*80}\")\n",
        "\n",
        "        for result in successful_results:\n",
        "            name = result['dataset_name']\n",
        "            metrics = result['results']\n",
        "            accuracy = metrics['accuracy']\n",
        "            precision = metrics['precision_weighted']\n",
        "            recall = metrics['recall_weighted']\n",
        "            f1 = metrics['f1_weighted']\n",
        "            depth = metrics['tree_complexity']['max_depth']\n",
        "            nodes = metrics['tree_complexity']['num_nodes']\n",
        "\n",
        "            print(f\"{name:<15} {accuracy:<10.4f} {precision:<10.4f} {recall:<10.4f} {f1:<10.4f} {depth:<7} {nodes:<7}\")\n",
        "\n",
        "        # Best performer\n",
        "        best_result = max(successful_results, key=lambda x: x['results']['accuracy'])\n",
        "        print(f\"\\n BEST ACCURACY: {best_result['dataset_name']} ({best_result['results']['accuracy']:.1%})\")\n",
        "\n",
        "        # Dataset analysis\n",
        "        print(f\"\\n DATASET COMPLEXITY ANALYSIS\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        for result in successful_results:\n",
        "            info = result['dataset_info']\n",
        "            tree_stats = result['results']['tree_complexity']\n",
        "\n",
        "            print(f\"\\n{result['dataset_name']}:\")\n",
        "            print(f\"  • Size: {info['shape'][0]:,} samples × {info['num_features']} features\")\n",
        "            print(f\"  • Classes: {info['num_classes']} classes\")\n",
        "            print(f\"  • Tree: {tree_stats['max_depth']} levels, {tree_stats['num_nodes']} nodes\")\n",
        "            print(f\"  • Performance: {result['results']['accuracy']:.1%} accuracy\")\n",
        "\n",
        "    if failed_results:\n",
        "        print(f\"\\n FAILED EVALUATIONS: {len(failed_results)}\")\n",
        "        for result in failed_results:\n",
        "            print(f\"  • {result['dataset_name']}: {result.get('error', 'Unknown error')}\")\n",
        "\n",
        "    print(f\"\\n{'='*100}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to test all datasets\"\"\"\n",
        "    print(f\" DECISION TREE EVALUATION - NUMPY FRAMEWORK\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\" Datasets to process: {len(datasets)}\")\n",
        "    print(f\" Framework: NumPy/sklearn\")\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    for i, dataset_info in enumerate(datasets, 1):\n",
        "        print(f\"\\n Processing {i}/{len(datasets)}: {dataset_info['name']}\")\n",
        "        result = test_single_dataset(dataset_info)\n",
        "        if result:\n",
        "            all_results.append(result)\n",
        "\n",
        "    # Print comprehensive summary\n",
        "    print_summary_report(all_results)\n",
        "\n",
        "    print(f\"\\n EVALUATION COMPLETE!\")\n",
        "    print(f\"Framework: NumPy\")\n",
        "    print(f\"Datasets processed: {len(all_results)}\")\n",
        "    print(f\"Successful: {len([r for r in all_results if r['success']])}\")\n",
        "\n",
        "    return all_results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = main()"
      ]
    }
  ]
}
